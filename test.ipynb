{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/anaconda3/envs/anhnct/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from einops import rearrange, repeat\n",
    "from trainer import train,validate,get_accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from util.log import Log\n",
    "from loader.dataloader import SkeletonAndEMGData\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from einops import rearrange, repeat\n",
    "from trainer import train,validate,get_accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from util.log import Log\n",
    "from loader.dataloader import SkeletonAndEMGData\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, max_position=512):\n",
    "        super(AbsolutePositionalEncoder, self).__init__()\n",
    "        self.position = torch.arange(max_position).unsqueeze(1)\n",
    "\n",
    "        self.positional_encoding = torch.zeros(1, max_position, emb_dim)\n",
    "\n",
    "        _2i = torch.arange(0, emb_dim, step=2).float()\n",
    "\n",
    "        # PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        self.positional_encoding[0, :, 0::2] = torch.sin(self.position / (10000 ** (_2i / emb_dim)))\n",
    "\n",
    "        # PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        self.positional_encoding[0, :, 1::2] = torch.cos(self.position / (10000 ** (_2i / emb_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch_size, input_len, embedding_dim\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        return self.positional_encoding[:batch_size, :seq_len, :]\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.att =  Attention(64, heads=8,\n",
    "                                        dim_head=64, dropout=0)\n",
    "        self.ff_layer = FeedForward(64, 64, dropout=0)\n",
    "        self.midLayerNorm = nn.LayerNorm(64, eps=1e-05)\n",
    "        self.outLayerNorm = nn.LayerNorm(64, eps=1e-05)\n",
    "    def forward(self,x):\n",
    "        hidden_states = self.att(x)\n",
    "        hidden_states = self.midLayerNorm(x+hidden_states)\n",
    "        # feed-forward layer\n",
    "        ff_hidden_states = self.ff_layer(hidden_states)\n",
    "\n",
    "        # add & norm layer\n",
    "        output_hidden_states = self.outLayerNorm(hidden_states+ff_hidden_states)\n",
    "        return output_hidden_states\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demo(nn.Module):\n",
    "    def __init__(self,device) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = ConvolutionComponent(3,512)\n",
    "        self.linear = nn.Linear(2,64)\n",
    "        self.position_encoding = AbsolutePositionalEncoder(64,512)\n",
    "        self.encoder = TransformerEncoderBlock()\n",
    "        self.LSTM = nn.LSTM(64,64,1,True,True)\n",
    "        self.downsample = nn.AdaptiveAvgPool1d(512)\n",
    "        self.ll = nn.Linear(8,64)\n",
    "        self.classify = nn.Linear(64,41)\n",
    "        self.device = device\n",
    "      \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.downsample(x)\n",
    "        old = rearrange(x,\" a b c -> a c b\")\n",
    "        x = torch.concat((x,torch.zeros((x.shape[0],1,x.shape[-1])).to(self.device)),dim = 1)\n",
    "        x = rearrange(x,\"b (p k) l -> b l p k\",p = 3)\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.position_encoding(x).to(self.device) + self.linear(x)\n",
    "        x = self.encoder(x)\n",
    "        x,_ = self.LSTM(x)\n",
    "        \n",
    "        ll = self.ll(old)\n",
    "        x = torch.sum(x,dim = 1).squeeze() + torch.sum(ll,dim = 1) + torch.rand((ll.shape[0],ll.shape[-1])).to(self.device)\n",
    "        return self.classify(x)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n",
      "3500\n",
      "3557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [11:48<00:00,  1.89s/it]\n",
      "100%|██████████| 112/112 [01:03<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- Train loss = 19.532836327855737 --- Valid loss = 8.15931681720727 -- Train set accuracy = 5.291666666666667 % Valid set Accuracy = 4.835535563677256 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [11:09<00:00,  1.79s/it]\n",
      "100%|██████████| 112/112 [01:00<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 --- Train loss = 6.115842882869872 --- Valid loss = 4.940386212665552 -- Train set accuracy = 6.158333333333333 % Valid set Accuracy = 5.7351700871520945 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [11:43<00:00,  1.88s/it]\n",
      "100%|██████████| 112/112 [01:00<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 --- Train loss = 5.1121047148754055 --- Valid loss = 4.604422955921739 -- Train set accuracy = 8.05 % Valid set Accuracy = 8.181051447849311 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [11:16<00:00,  1.80s/it]\n",
      "100%|██████████| 112/112 [00:59<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(20)\n",
    "\n",
    "data = []\n",
    "data.extend(glob.glob('data/new_data/val2/*'))\n",
    "data.extend(glob.glob('data/new_data/train2/*'))\n",
    "data.extend(glob.glob('data/new_data/test2/*'))\n",
    "\n",
    "\n",
    "index = np.random.permutation(len(data))\n",
    "data = np.array(data)[index]\n",
    "train_size = 12000\n",
    "test_size = 3500\n",
    "val_size = data.shape[0]-train_size - test_size\n",
    "trainset = data[:train_size]\n",
    "testset = data[train_size:train_size+test_size]\n",
    "valset = data[train_size+test_size:]\n",
    "\n",
    "train_set = SkeletonAndEMGData(trainset)\n",
    "test_set = SkeletonAndEMGData(testset)\n",
    "val_set = SkeletonAndEMGData(valset)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32,\n",
    "                          drop_last=False, num_workers=10)\n",
    "valid_loader = DataLoader(val_set, batch_size=32,\n",
    "                          drop_last=False, num_workers=10)\n",
    "test_loader = DataLoader(test_set, batch_size=32,\n",
    "                         drop_last=False, num_workers=10)\n",
    "\n",
    "model = Demo(device).to(device).double()\n",
    "\n",
    "log = Log(\"log/InceptionNet\", \"vit_emg\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 500\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "train_f1score_weighted = []\n",
    "val_f1scroe_weighted = []\n",
    "\n",
    "train_f1score_micro = []\n",
    "val_f1scroe_micro = []\n",
    "\n",
    "test_log = []\n",
    "\n",
    "best_f1 = -1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    model, train_loss, optimizer = train(\n",
    "        train_loader, model, criterion, optimizer, device)\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        model, valid_loss = validate(valid_loader, model, criterion, device)\n",
    "    train_acc, f1_score_weighted, f1_score_micro = get_accuracy(\n",
    "        model, train_loader, device)\n",
    "    # save f1 score\n",
    "    train_f1score_weighted.append(f1_score_weighted)\n",
    "    train_f1score_micro.append(f1_score_micro)\n",
    "\n",
    "    val_acc, f1_score_weighted, f1_score_micro = get_accuracy(\n",
    "        model, valid_loader, device)\n",
    "    # save f1 score\n",
    "    if best_f1 < f1_score_micro:\n",
    "        torch.save(model.state_dict(),\n",
    "                   f\"log/InceptionNet/best_model{epoch}.pth\")\n",
    "        log.save_model(model)\n",
    "        best_f1 = f1_score_micro\n",
    "    val_f1scroe_weighted.append(f1_score_weighted)\n",
    "    val_f1scroe_micro.append(f1_score_micro)\n",
    "    print(\"Epoch {} --- Train loss = {} --- Valid loss = {} -- Train set accuracy = {} % Valid set Accuracy = {} %\".format\n",
    "          (epoch+1, train_loss, valid_loss, train_acc, val_acc))\n",
    "    # save loss value\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    # save accuracy\n",
    "    train_accuracy.append(train_acc)\n",
    "    val_accuracy.append(val_acc)\n",
    "\n",
    "    test_log.append(get_accuracy(model, test_loader, device))\n",
    "\n",
    "    log.save_training_log(train_losses, train_accuracy,\n",
    "                          train_f1score_weighted, train_f1score_micro)\n",
    "    log.save_val_log(valid_losses, val_accuracy,\n",
    "                     val_f1scroe_weighted, val_f1scroe_micro)\n",
    "    log.save_test_log(test_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anhnct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
